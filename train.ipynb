{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556145b4-f68b-4c5f-9992-047f755fccfd",
   "metadata": {},
   "source": [
    "##  Load packages and configure GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7335e6-97fc-4ddc-a74e-a31be968b877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cc7790-1c0f-4631-83bc-f2c2f5a7bd45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mainuser/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/mainuser/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os, sys\n",
    "import warnings\n",
    "from cholect50 import dataloader_tf as dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#os.environ['PYTHONPATH'] += \":/code/models\"\n",
    "\n",
    "#import tensorflow_models as tfm\n",
    "#from tensorflow_models import vision\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, ConvLSTM2D, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84896caf-3d8a-468e-a520-b36783385972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "149a915e-77f6-4de2-b64d-3597a638eb54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[1],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024*24)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49174f85-0e0d-4e3a-97de-2954a634da0f",
   "metadata": {},
   "source": [
    "## Load train, test, val dataset using the Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13ad6cee-ee3c-4789-8925-447dc13ece77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e938457a-6d1b-48d9-8470-358c0fb90c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataloader.CholecT50( \n",
    "          dataset_dir=\"CholecT50\", \n",
    "          dataset_variant=\"cholect50\",\n",
    "          test_fold=1,\n",
    "          augmentation_list=['original'],\n",
    "          num_parallel_calls=100\n",
    "          )\n",
    "\n",
    "# build dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.build()\n",
    "\n",
    "# train and val data loaders\n",
    "train_dataloader = train_dataset.batch(BATCH_SIZE) # see tf.data.Dataset for more options\n",
    "val_dataloader   = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# test data set is built per video, so load differently\n",
    "test_dataloaders = []\n",
    "for video_dataset in test_dataset:\n",
    "    test_dataloader = video_dataset.batch(BATCH_SIZE)\n",
    "    test_dataloaders.append(test_dataloader)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b94990-1277-4a6f-b6ee-f103f93a50c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d82f4-72ec-496c-98d6-ee918c74c313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "instruments = [\n",
    "    \"Grasper\",\n",
    "    \"Bipolar\",\n",
    "    \"Hook\",\n",
    "    \"Scissors\",\n",
    "    \"Clipper\",\n",
    "    \"Irrigator\"\n",
    "]\n",
    "\n",
    "for (img, (ivt, i, v, t, p)) in train_dataloader.take(1):\n",
    "    l = [x==1 for x in i.numpy()]\n",
    "    \n",
    "lis = np.array(instruments)\n",
    "fil = np.array(l)\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 10))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        axs[i, j].imshow((img.numpy()[i*2+j]).astype(np.uint8))\n",
    "        axs[i, j].set_title(str(lis[fil[i*2+j]]))\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0.2)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf7ccb-f367-4386-b75f-a45f00795c2d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caa74100-dc42-4763-a09e-db57c1d32a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "NUM_FRAMES = 16\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "class ReshapeLayer(Layer):\n",
    "    def call(self,inputs):\n",
    "        nshape = (BATCH_SIZE,NUM_FRAMES) + inputs.shape[1:]\n",
    "        return tf.reshape(inputs,nshape)\n",
    "    \n",
    "class ExpandLayer(Layer):\n",
    "    def call(self,inputs):\n",
    "        nshape = (BATCH_SIZE,NUM_FRAMES) + inputs.shape[1:]\n",
    "        return tf.reshape(inputs,nshape)\n",
    "    \n",
    "def wildcat_pooling(img, alpha=0.6, name='Wildcat_Pooling'):\n",
    "    # Axis: Breadth and Width of the input tensor. Assuming\n",
    "    # 0 is the batch size. Check if we have a 5D Tensor\n",
    "    with tf.name_scope(name):\n",
    "        return tf.math.reduce_max(img, axis=[-3,-2]) + alpha*tf.math.reduce_min(img, axis=[-3,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83d5dbe4-0cc6-4a9c-9532-6b76e2aa3f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]], shape=(8, 6), dtype=float32)\n",
      "[256 448   3]\n"
     ]
    }
   ],
   "source": [
    "for (img, (ivt, i, v, t, p)) in train_dataloader.take(1):\n",
    "    pass\n",
    "\n",
    "input_shape = np.array(img[0].shape)\n",
    "print(i)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8512e-3f47-40ab-98ce-bd352f4bfe52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Temp Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9eb673bf-0940-4a2e-b6c4-0a5acebb140f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m         X \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mResidualBlock(filters[i], strides[i], use_projection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(X)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mvision\u001b[49m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mResidualBlock(filters[i], strides[i], use_projection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)(X)\n\u001b[1;32m     29\u001b[0m         X \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mResidualBlock(filters[i], strides[i], use_projection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)(X)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"        \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mX = tf.keras.applications.resnet50.ResNet50(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m)(X_input)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vision' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the input as a tensor with shape input_shape\n",
    "filters = [64, 128, 256, 512]\n",
    "strides = [1,   2,   1,   1]\n",
    "\n",
    "X_input = Input(shape = (256, 448, 3))\n",
    "\n",
    "\n",
    "# Zero-Padding\n",
    "X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "\n",
    "#[Note]: Using ResNet50-V1 instead \n",
    "\n",
    "\n",
    "# Stage 1\n",
    "X = Conv2D(64, (7, 7), \n",
    "           strides=(2, 2), \n",
    "           name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "X = BatchNormalization(name='bnConv1')(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "for i in range(len(filters)):\n",
    "    if (i > 0): \n",
    "        X = vision.layers.ResidualBlock(filters[i], strides[i], use_projection=True)(X)\n",
    "        X = vision.layers.ResidualBlock(filters[i], strides[i], use_projection=True)(X)\n",
    "    else:\n",
    "        X = vision.layers.ResidualBlock(filters[i], strides[i], use_projection=False)(X)\n",
    "        X = vision.layers.ResidualBlock(filters[i], strides[i], use_projection=False)(X)\n",
    "\"\"\"        \n",
    "\n",
    "X = tf.keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(256, 448, 3),\n",
    "    include_top=False\n",
    ")(X_input)\n",
    "\"\"\"\n",
    "\n",
    "reshaped_input = ReshapeLayer()(X)\n",
    "\n",
    "# # Return sequence True will return all the sequence of images\n",
    "# # if set to false, it will only return the last image \n",
    "X = ConvLSTM2D(\n",
    "     filters=6,\n",
    "     kernel_size=(1, 1), \n",
    "     name='convLSTMLayer', \n",
    "     kernel_regularizer = regularizers.L2(1e-5),\n",
    "     return_sequences=True)(reshaped_input)\n",
    "\n",
    "# Wildcat Pooling goes here\n",
    "\n",
    "# Need to move after ConvLSTM in final\n",
    "X = Conv2D(filters=6, kernel_size=(1, 1), name='LocMapLayer')(X)\n",
    "\n",
    "\n",
    "model = Model(inputs=X_input, outputs=X, name='WNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd0f3a-a511-4c05-979d-14905fedf90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927342e5-ea04-49e8-be27-af0d5730bfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/qubvel/classification_models.git\n",
      "  Cloning https://github.com/qubvel/classification_models.git to /tmp/pip-req-build-nf17aht7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/qubvel/classification_models.git /tmp/pip-req-build-nf17aht7\n",
      "  Resolved https://github.com/qubvel/classification_models.git to commit a0f006e05485a34ccf871c421279864b0ccd220b\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: keras_applications<=1.0.8,>=1.0.7 in /home/mainuser/.local/lib/python3.8/site-packages (from image-classifiers==1.0.0) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/mainuser/.local/lib/python3.8/site-packages (from keras_applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (1.23.5)\n",
      "Requirement already satisfied: h5py in /home/mainuser/.local/lib/python3.8/site-packages (from keras_applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (3.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/qubvel/classification_models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e4f76d-4848-49d2-9436-6c7305743179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### WITH RESNET18 PRETRAINED\n",
    "from classification_models.keras import Classifiers\n",
    "\n",
    "ResNet18, preprocess_input = Classifiers.get('resnet18')\n",
    "\n",
    "X_input = Input(shape = (256, 448, 3))\n",
    "\n",
    "X = ResNet18(input_shape=(256, 448, 3), weights='imagenet', include_top=False)(X_input)\n",
    "\n",
    "\n",
    "reshaped_input = ReshapeLayer()(X)\n",
    "\n",
    "# # Return sequence True will return all the sequence of images\n",
    "# # if set to false, it will only return the last image \n",
    "# X = ConvLSTM2D(\n",
    "#     filters=6,\n",
    "#     kernel_size=(1, 1), \n",
    "#     name='convLSTMLayer', \n",
    "#     kernel_regularizer = regularizers.L2(1e-5),\n",
    "#     return_sequences=True)(reshaped_input)\n",
    "\n",
    "X = ConvLSTM2D(\n",
    "     filters=6,\n",
    "     kernel_size=(1, 1), \n",
    "     name='convLSTMLayer', \n",
    "     kernel_regularizer = regularizers.L2(1e-5),\n",
    "     return_sequences=True)(reshaped_input)\n",
    "\n",
    "# Wildcat Pooling goes here\n",
    "\n",
    "# Need to move after ConvLSTM in final\n",
    "X = Conv2D(filters=6, kernel_size=(1, 1), name='LocMapLayer')(X)\n",
    "\n",
    "\n",
    "model = Model(inputs=X_input, outputs=X, name='WNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9b4aaf5-8481-4a3d-9b17-80d1c33596d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"WNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 448, 3)]     0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 8, 14, 512)        11186889  \n",
      "                                                                 \n",
      " reshape_layer (ReshapeLayer  (8, 16, 8, 14, 512)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " convLSTMLayer (ConvLSTM2D)  (8, 16, 8, 14, 6)         12456     \n",
      "                                                                 \n",
      " LocMapLayer (Conv2D)        (8, 16, 8, 14, 6)         42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,199,387\n",
      "Trainable params: 11,191,445\n",
      "Non-trainable params: 7,942\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ef0f122-99f1-4f3e-8049-d8a53c371eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_model_for_demo = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(256, 448, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255d3dc-8694-4a6d-ae1e-61a650be5174",
   "metadata": {},
   "source": [
    "## Model v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17e3840b-755f-4b19-9448-5bfb4e020d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"WNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 16, 256, 44  0           []                               \n",
      "                                8, 3)]                                                            \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 16, 8, 14, 5  11186889   ['input_11[0][0]']               \n",
      " buted)                         12)                                                               \n",
      "                                                                                                  \n",
      " convLSTMLayer (ConvLSTM2D)     (None, 8, 14, 6)     12456       ['time_distributed_2[0][0]']     \n",
      "                                                                                                  \n",
      " LocMapLayer (Conv2D)           (None, 8, 14, 6)     42          ['convLSTMLayer[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.reduce_min_4 (TFOpLamb  (None, 6)           0           ['LocMapLayer[0][0]']            \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_4 (TFOpLamb  (None, 6)           0           ['LocMapLayer[0][0]']            \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 6)           0           ['tf.math.reduce_min_4[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 6)           0           ['tf.math.reduce_max_4[0][0]',   \n",
      " mbda)                                                            'tf.math.multiply_4[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,199,387\n",
      "Trainable params: 11,191,445\n",
      "Non-trainable params: 7,942\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from classification_models.keras import Classifiers\n",
    "ResNet18, preprocess_input = Classifiers.get('resnet18')\n",
    "resnet = ResNet18(input_shape=(256, 448, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "X_input = Input(shape = (16, 256, 448, 3))\n",
    "reshaped_input = TimeDistributed(resnet)(X_input)\n",
    "X = ConvLSTM2D(\n",
    "     filters=6,\n",
    "     kernel_size=(1, 1), \n",
    "     name='convLSTMLayer', \n",
    "     kernel_regularizer = regularizers.L2(1e-5),\n",
    "     return_state=False)(reshaped_input)\n",
    "\n",
    "X = Conv2D(filters=6, kernel_size=(1, 1), name='LocMapLayer')(X)\n",
    "X = wildcat_pooling(X)\n",
    "\n",
    "model = Model(inputs=X_input, outputs=X, name='WNet')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351373bb-3905-4497-9a84-9bf44ffc2a20",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2959dd96-8e74-45b4-89b0-eac17f053282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.nn import weighted_cross_entropy_with_logits as loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa252411-7b7b-403e-a34e-ecb6976027e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_weights = tf.convert_to_tensor(\n",
    "    [0.08084519, 0.81435289, 0.10459284, 2.55976864, 1.630372490, 1.29528455], \n",
    "    dtype=tf.float32, dtype_hint=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ab30395-0fb0-4f7f-8f5e-ece3cfdc338e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-1),\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "]\n",
    "optimizers_and_layers = [(optimizers[0], model.layers[0:2]), (optimizers[1], model.layers[2:])]\n",
    "optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "65527d67-264d-4013-aa53-7c2a12ba78d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c86e635c-5503-4986-8f51-f836051ff92a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"WNet\" is incompatible with the layer: expected shape=(None, 16, 256, 448, 3), found shape=(8, 256, 448, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 17\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (img, (_, label_i, _, _, _)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      8\u001b[0m            \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Open a GradientTape to record the operations run\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# during the forward pass, which enables auto-differentiation.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# Run the forward pass of the layer.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# The operations that the layer applies\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# to its inputs are going to be recorded\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# on the GradientTape.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Logits for this minibatch\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m#print(label_i)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m#label_i=tf.cast(label_i,tf.float32)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# Compute the loss value for this minibatch.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         loss_value \u001b[38;5;241m=\u001b[39m loss_fn(labels\u001b[38;5;241m=\u001b[39mlabel_i, logits\u001b[38;5;241m=\u001b[39mlogits, pos_weight\u001b[38;5;241m=\u001b[39mclass_weights)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/input_spec.py:264\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    265\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    266\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    267\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"WNet\" is incompatible with the layer: expected shape=(None, 16, 256, 448, 3), found shape=(8, 256, 448, 3)"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (img, (_, label_i, _, _, _)) in enumerate(train_dataloader):\n",
    "               \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(img, training=True)  # Logits for this minibatch\n",
    "            \n",
    "            #print(label_i)\n",
    "            \n",
    "            #label_i=tf.cast(label_i,tf.float32)\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(labels=label_i, logits=logits, pos_weight=class_weights)\n",
    "            #loss_value = tf.expand_dims(tf.math.reduce_mean(loss_value, axis = -1),axis = -1)\n",
    "            loss_value = tf.math.reduce_mean(loss_value)\n",
    "            #print(loss_value)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 2 batches.\n",
    "        if step % 1000 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98a76a-9231-4b4d-b0dc-9c1ce7fc49e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

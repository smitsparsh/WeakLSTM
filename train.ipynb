{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556145b4-f68b-4c5f-9992-047f755fccfd",
   "metadata": {},
   "source": [
    "##  Load packages and configure GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e7335e6-97fc-4ddc-a74e-a31be968b877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-models-official (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-models-official\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cc7790-1c0f-4631-83bc-f2c2f5a7bd45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PYTHONPATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYTHONPATH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:/code/models\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_models\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfm\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vision\n",
      "File \u001b[0;32m/usr/lib/python3.8/os.py:675\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    672\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PYTHONPATH'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os, sys\n",
    "import warnings\n",
    "from cholect50 import dataloader_tf as dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "os.environ['PYTHONPATH'] += \":/code/models\"\n",
    "\n",
    "import tensorflow_models as tfm\n",
    "from tensorflow_models import vision\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, ConvLSTM2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a915e-77f6-4de2-b64d-3597a638eb54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[1],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024*24)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49174f85-0e0d-4e3a-97de-2954a634da0f",
   "metadata": {},
   "source": [
    "## Load train, test, val dataset using the Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad6cee-ee3c-4789-8925-447dc13ece77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938457a-6d1b-48d9-8470-358c0fb90c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataloader.CholecT50( \n",
    "          dataset_dir=\"CholecT50\", \n",
    "          dataset_variant=\"cholect50\",\n",
    "          test_fold=1,\n",
    "          augmentation_list=['original'],\n",
    "          num_parallel_calls=100\n",
    "          )\n",
    "\n",
    "# build dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.build()\n",
    "\n",
    "# train and val data loaders\n",
    "train_dataloader = train_dataset.batch(BATCH_SIZE) # see tf.data.Dataset for more options\n",
    "val_dataloader   = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# test data set is built per video, so load differently\n",
    "test_dataloaders = []\n",
    "for video_dataset in test_dataset:\n",
    "    test_dataloader = video_dataset.batch(BATCH_SIZE)\n",
    "    test_dataloaders.append(test_dataloader)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b94990-1277-4a6f-b6ee-f103f93a50c4",
   "metadata": {},
   "source": [
    "## Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d82f4-72ec-496c-98d6-ee918c74c313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "instruments = [\n",
    "    \"Grasper\",\n",
    "    \"Bipolar\",\n",
    "    \"Hook\",\n",
    "    \"Scissors\",\n",
    "    \"Clipper\",\n",
    "    \"Irrigator\"\n",
    "]\n",
    "\n",
    "for (img, (ivt, i, v, t, p)) in train_dataloader.take(1):\n",
    "    l = [x==1 for x in i.numpy()]\n",
    "    \n",
    "lis = np.array(instruments)\n",
    "fil = np.array(l)\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 10))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        axs[i, j].imshow((img.numpy()[i*2+j]).astype(np.uint8))\n",
    "        axs[i, j].set_title(str(lis[fil[i*2+j]]))\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0.2)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf7ccb-f367-4386-b75f-a45f00795c2d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa74100-dc42-4763-a09e-db57c1d32a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/63860100/reshape-a-4d-tensor-output-of-a-convolutional-layer-to-5d-tensor-to-be-fed-to-a\n",
    "\n",
    "NUM_FRAMES = 16\n",
    "class ReshapeLayer(Layer):\n",
    "    def call(self,inputs):\n",
    "        nshape = (BATCH_SIZE, NUM_FRAMES) + inputs.shape[1:]\n",
    "        return tf.reshape(inputs,nshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5dbe4-0cc6-4a9c-9532-6b76e2aa3f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for (img, (ivt, i, v, t, p)) in train_dataloader.take(1):\n",
    "    pass\n",
    "\n",
    "input_shape = np.array(img[0].shape)\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb673bf-0940-4a2e-b6c4-0a5acebb140f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the input as a tensor with shape input_shape\n",
    "filters = [64, 128, 256, 512]\n",
    "strides = [1,   2,   1,   1]\n",
    "\n",
    "X_input = Input(shape = (256, 448, 3))\n",
    "\n",
    "\n",
    "# Zero-Padding\n",
    "X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "\n",
    "#[Note]: Using ResNet50-V1 instead \n",
    "\n",
    "\n",
    "# Stage 1\n",
    "X = Conv2D(64, (7, 7), \n",
    "           strides=(2, 2), \n",
    "           name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "X = BatchNormalization(name='bnConv1')(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "for i in range(len(filters)):\n",
    "    if (i > 0): \n",
    "        X = vision.layers.ResidualBlock(filters[i], strides[i], use_projection=True)(X)\n",
    "        X = vision.layers.ResidualBlock(filters[i], strides[i], use_projection=True)(X)\n",
    "    else:\n",
    "        X = vision.layers.ResidualBlock(filters[i], strides[i], use_projection=False)(X)\n",
    "        X = vision.layers.ResidualBlock(filters[i], strides[i], use_projection=False)(X)\n",
    "\"\"\"        \n",
    "\n",
    "X = tf.keras.applications.resnet50.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(256, 448, 3),\n",
    "    include_top=False\n",
    ")(X_input)\n",
    "\"\"\"\n",
    "\n",
    "reshaped_input = ReshapeLayer()(X)\n",
    "\n",
    "# # Return sequence True will return all the sequence of images\n",
    "# # if set to false, it will only return the last image \n",
    "X = ConvLSTM2D(\n",
    "     filters=6,\n",
    "     kernel_size=(1, 1), \n",
    "     name='convLSTMLayer', \n",
    "     kernel_regularizer = regularizers.L2(1e-5),\n",
    "     return_sequences=True)(reshaped_input)\n",
    "\n",
    "# Wildcat Pooling goes here\n",
    "\n",
    "# Need to move after ConvLSTM in final\n",
    "X = Conv2D(filters=6, kernel_size=(1, 1), name='LocMapLayer')(X)\n",
    "\n",
    "\n",
    "model = Model(inputs=X_input, outputs=X, name='WNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd0f3a-a511-4c05-979d-14905fedf90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927342e5-ea04-49e8-be27-af0d5730bfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/qubvel/classification_models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4f76d-4848-49d2-9436-6c7305743179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### WITH RESNET18 PRETRAINED\n",
    "from classification_models.keras import Classifiers\n",
    "\n",
    "ResNet18, preprocess_input = Classifiers.get('resnet18')\n",
    "\n",
    "X_input = Input(shape = (256, 448, 3))\n",
    "\n",
    "X = ResNet18(input_shape=(256, 448, 3), weights='imagenet', include_top=False)(X_input)\n",
    "\n",
    "\n",
    "# reshaped_input = ReshapeLayer()(X)\n",
    "\n",
    "# # Return sequence True will return all the sequence of images\n",
    "# # if set to false, it will only return the last image \n",
    "# X = ConvLSTM2D(\n",
    "#     filters=6,\n",
    "#     kernel_size=(1, 1), \n",
    "#     name='convLSTMLayer', \n",
    "#     kernel_regularizer = regularizers.L2(1e-5),\n",
    "#     return_sequences=True)(reshaped_input)\n",
    "\n",
    "# Wildcat Pooling goes here\n",
    "\n",
    "# Need to move after ConvLSTM in final\n",
    "X = Conv2D(filters=6, kernel_size=(1, 1), name='LocMapLayer')(X)\n",
    "\n",
    "\n",
    "model = Model(inputs=X_input, outputs=X, name='WNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4aaf5-8481-4a3d-9b17-80d1c33596d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351373bb-3905-4497-9a84-9bf44ffc2a20",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959dd96-8e74-45b4-89b0-eac17f053282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.nn import weighted_cross_entropy_with_logits as loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa252411-7b7b-403e-a34e-ecb6976027e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_weights = [0.08084519, 0.81435289, 0.10459284, 2.55976864, 1.630372490, 1.29528455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab30395-0fb0-4f7f-8f5e-ece3cfdc338e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-1),\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "]\n",
    "optimizers_and_layers = [(optimizers[0], model.layers[0:2]), (optimizers[1], model.layers[2:])]\n",
    "optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebce8d1-f2e1-4461-bc00-a9762591146b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e635c-5503-4986-8f51-f836051ff92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for iteration, (img, (_, label_i, _, _, _)) in enumerate(train_dataloader):\n",
    "               \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(img, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(labels=label_i, logits=logits, pos_weight=class_weights)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de714e78-5b94-483a-9029-454c1546ff7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch, (img, (_, label_i, _, _, _)) in enumerate(train_dataloader):\n",
    "    print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63967402-55d0-4c1d-9ff8-06d4f19d5a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
